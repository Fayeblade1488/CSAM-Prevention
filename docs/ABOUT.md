**What:** CSAM Guard is a production-oriented API designed to detect and prevent child-safety risk signals in text and images. It employs a multi-layered approach that includes heuristics, optional NLP scoring, and image perceptual hash matching to identify potentially harmful content.

**Why:** The project aims to provide a reliable and efficient solution for platforms to proactively identify and mitigate the risks associated with child sexual abuse material (CSAM).

**Status:** beta

**Who:** @org/maintainers, @org/sec
